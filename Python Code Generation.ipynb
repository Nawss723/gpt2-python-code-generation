{"cells":[{"cell_type":"markdown","source":["\n","\n","# This project leverages the GPT-2 model, a transformer-based language model developed by OpenAI, to generate Python code\n","**1.Installing Required Libraries**\n","\n","\n","*   PyGithub: Used for interacting with the GitHub API.\n","*   pyarrow, datasets, and transformers: These libraries are essential for loading datasets and working with pre-trained models from the Hugging Face library.\n","\n","\n"],"metadata":{"id":"La3D4VVQtTnM"},"id":"La3D4VVQtTnM"},{"cell_type":"code","source":["!pip install PyGithub\n","!pip install pyarrow==14.0.1 datasets==2.12.0 transformers==4.31.0\n"],"metadata":{"id":"ifxRb2L9tScH"},"id":"ifxRb2L9tScH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2.Loading the GPT-2 Tokenizer and Model**\n","\n","\n","* AutoTokenizer: This helps convert the input text into tokens that the model can understand.\n","* AutoModelForCausalLM: GPT-2 is a pre-trained causal language model, meaning it is trained to predict the next token in a sequence of tokens, which is useful for generating text or code.\n","\n"],"metadata":{"id":"-jgMjcM2tomQ"},"id":"-jgMjcM2tomQ"},{"cell_type":"code","source":["tokenizer.pad_token = tokenizer.eos_token\n"],"metadata":{"id":"xvHJAdv2tn_8"},"id":"xvHJAdv2tn_8","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**3. Mounting Google Drive to Access the Dataset**\n","\n","\n","* This command mounts Google Drive to the Colab environment, allowing you to access files stored in your drive.\n","\n","\n"],"metadata":{"id":"FN8JLU52t9mg"},"id":"FN8JLU52t9mg"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"i027txWot8ii"},"id":"i027txWot8ii","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4.Loading and Preparing the Dataset**\n","\n","* pandas: Used to load the dataset from a CSV file into a DataFrame\n","* Dataset.from_pandas: Converts the pandas DataFrame into a Hugging Face Dataset object, which is more efficient for training models with the transformers library.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ggwUADSDuLr-"},"id":"ggwUADSDuLr-"},{"cell_type":"code","source":["import pandas as pd\n","from datasets import Dataset\n","\n","csv_file_path = 'python_code_dataset.csv' #csv path\n","df = pd.read_csv(csv_file_path)\n","dataset = Dataset.from_pandas(df)\n"],"metadata":{"id":"LOpkloemuY5V"},"id":"LOpkloemuY5V","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**5.Preprocessing the Dataset**\n","\n","\n","* Purpose: This function processes each row in the dataset by combining two fields: instruction and input. If any field is missing (NaN), it is replaced with an empty string.\n","* Tokenization: The combined text is tokenized, and the tokenized version is truncated to a maximum of 128 tokens. The labels are also set to the tokenized input IDs (used for model training).\n","\n"],"metadata":{"id":"Os_wfbrHuctH"},"id":"Os_wfbrHuctH"},{"cell_type":"code","source":["def preprocess_function(examples):\n","    combined_texts = []\n","    for instruction, input_text in zip(examples['instruction'], examples['input']):\n","        instruction = instruction if pd.notna(instruction) else \"\"\n","        input_text = input_text if pd.notna(input_text) else \"\"\n","        combined_text = f\"{instruction} {input_text}\"\n","        combined_texts.append(combined_text)\n","\n","    tokenized_inputs = tokenizer(combined_texts, truncation=True, max_length=128, padding='max_length')\n","    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n","\n","    return tokenized_inputs\n"],"metadata":{"id":"b4tRunEduldL"},"id":"b4tRunEduldL","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**6.Tokenizing the Dataset**\n","\n","* map: Applies the preprocess_function to every example in the dataset. The function processes the examples in batches, which improves efficiency.\n","\n","\n","\n"],"metadata":{"id":"S1iX-uVeupSQ"},"id":"S1iX-uVeupSQ"},{"cell_type":"code","source":["tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"],"metadata":{"id":"KxpQgkvSuyI9"},"id":"KxpQgkvSuyI9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**7. Splitting the Dataset into Train and Test Sets**\n","\n","* train_test_split: Splits the dataset into two parts, with 90% used for training and 10% reserved for testing the model.\n","\n","\n"],"metadata":{"id":"E1ZaDZRqu-Ja"},"id":"E1ZaDZRqu-Ja"},{"cell_type":"code","source":["tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n"],"metadata":{"id":"Qmrd2aQFvJo4"},"id":"Qmrd2aQFvJo4","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**8. Defining Training Arguments**\n","\n","\n","* output_dir: Where the model checkpoints and results will be saved.\n","* per_device_train_batch_size: Sets the batch size to 1 per device (which is necessary for limited memory environments like Colab).\n","* num_train_epochs: The number of full passes through the training dataset (in this case, 1 epoch).\n","* save_steps: Saves the model after every 5000 training steps\n","* save_total_limit: Limits the number of saved models to avoid running out of storage.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"691N3re3vNkM"},"id":"691N3re3vNkM"},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    per_device_train_batch_size=1,\n","    num_train_epochs=1,\n","    save_steps=5000,\n","    save_total_limit=1,\n",")\n"],"metadata":{"id":"TzPScIl7viu0"},"id":"TzPScIl7viu0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**9. Training the Model**\n","\n","\n","*   Trainer: This is the Hugging Face utility that handles the entire training loop, from feeding data into the model to updating the weights. It simplifies the training process.\n","*   The model is trained on the train portion of the tokenized dataset.\n","\n"],"metadata":{"id":"r3zME62SvlB6"},"id":"r3zME62SvlB6"},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets['train'],\n",")\n","\n","trainer.train()\n"],"metadata":{"id":"pNrvBvOdvvrq"},"id":"pNrvBvOdvvrq","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.Generating Code Using the Fine-Tuned Model**\n","\n","\n","*  generate_code: This function takes a prompt (for example, the start of a Python function) and uses the fine-tuned model to generate a continuation of that code.\n","*  max_length=500: The model can generate up to 500 tokens in the output.\n","\n","*  The generated output is tokenized back into readable text using the tokenizer.decode function, which removes any special tokens used during training.\n","\n"],"metadata":{"id":"lySnuaVEvy3r"},"id":"lySnuaVEvy3r"},{"cell_type":"code","source":["def generate_code(prompt, max_length=500):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    outputs = model.generate(inputs['input_ids'], max_length=max_length)\n","    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return generated_code\n"],"metadata":{"id":"MoP1Pnp9wCTp"},"id":"MoP1Pnp9wCTp","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**11.Testing the Model with a Prompt**\n","\n","\n","*  Here, a sample prompt for a function (merge_sort) is provided to the model, and the model generates the remaining part of the function based on its training.\n","*  The generated code is printed\n","\n"],"metadata":{"id":"TskophHZwFDp"},"id":"TskophHZwFDp"},{"cell_type":"code","source":["prompt = \"def merge_sort(arr):\"\n","generated_code = generate_code(prompt)\n","\n","print(\"Generated Code:\")\n","print(generated_code)\n"],"metadata":{"id":"H97973ajwOng"},"id":"H97973ajwOng","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}